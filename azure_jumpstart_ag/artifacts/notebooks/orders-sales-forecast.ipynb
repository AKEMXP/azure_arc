{"cells":[{"cell_type":"markdown","id":"7a59fda1-4e93-45fb-8d19-7db1a882cc5e","metadata":{"jupyter":{"magics_cell_name":"magics-cell-markdown","magics_signature":"27ac753c3c60167f65c4d05fa7809cd85f1f0273d5b842aca4f65a01"},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["\n","#### Run the cell below to install the required packages for Copilot\n"]},{"cell_type":"code","execution_count":null,"id":"201b7702-02d9-4d89-abc7-0cdbf6277f24","metadata":{"jupyter":{"magics_cell_name":"magics-cell-code","magics_signature":"6565d62221c469ab3707694ccbef2e4568d575dc1ba3ebac23f0f052","magics_version":"1.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["\n","#Run this cell to install the required packages for Copilot\n","%load_ext dscopilot_installer\n","%activate_dscopilot\n"]},{"cell_type":"markdown","id":"ef02f63e-941b-42c0-8481-ce8aac727fa8","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Contoso Hypermarket Orders sales forecast ML notebook"]},{"cell_type":"markdown","id":"b1418874-4b23-4dae-8748-07e756bf70f4","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Set up MLflow experiment tracking"]},{"cell_type":"code","execution_count":null,"id":"edac2e41-e4ec-4a84-adad-d57938e17562","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Set up MLflow for experiment tracking\n","import mlflow\n","\n","IS_SAMPLE = False  # if TRUE, use only rows of data for training, otherwise use all data\n","SAMPLE_ROWS = 5000  # if IS_SAMPLE is True, use only this number of rows for training\n","EXPERIMENT_NAME = \"orders-sales-forecast\"  # MLflow experiment name\n","\n","mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(disable=True)  # Disable MLflow autologging"]},{"cell_type":"markdown","id":"9fa68416-1c4d-4481-8ec2-c2571310afbf","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Load orders data from KQL database to prepare for ML modeling\n","\n","> [!IMPORTANT]\n","> Make sure you have enough data generated using data emulator."]},{"cell_type":"code","execution_count":null,"id":"941a85a3-eaf6-4f2d-b862-2822a11cba86","metadata":{"jupyter":{"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Read from Kusto\n","# kustoQuery = \"['orders'] |  mv-expand li = parse_json(line_items) | project order_date, store_id, order_id, product_id = toint(li.product_id), quantity = toint(li.quantity), price = toreal(li.price), item_total = toreal(li.item_total), order_total\"\n","ordersQuery = \"['orders'] |  mv-expand li = parse_json(line_items) | project order_date, store_id, order_id, product_id = toint(li.product_id), quantity = toint(li.quantity), price = toreal(li.price), item_total = toreal(li.item_total), order_total\"\n","inventoryQuery = \"['inventory'] | project date_time, store_id, product_id, in_stock, retail_price\"\n","productsQuery = \"['products'] | project product_id, name, category, photo_path, price_range, stock\"\n","# The query URI for reading the data e.g. https://<>.kusto.data.microsoft.com.\n","kustoUri = \"https://trd-g8jnmstet4k7q79z9v.z1.kusto.fabric.microsoft.com\"\n","# The database with data to be read.\n","database = \"contosohypermarket\"\n","# The access credentials.\n","accessToken = mssparkutils.credentials.getToken(kustoUri)\n","ordersDf  = spark.read\\\n","    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n","    .option(\"accessToken\", accessToken)\\\n","    .option(\"kustoCluster\", kustoUri)\\\n","    .option(\"kustoDatabase\", database)\\\n","    .option(\"kustoQuery\", ordersQuery).load()\n","inventoryDf  = spark.read\\\n","    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n","    .option(\"accessToken\", accessToken)\\\n","    .option(\"kustoCluster\", kustoUri)\\\n","    .option(\"kustoDatabase\", database)\\\n","    .option(\"kustoQuery\", inventoryQuery).load()\n","productsDf  = spark.read\\\n","    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n","    .option(\"accessToken\", accessToken)\\\n","    .option(\"kustoCluster\", kustoUri)\\\n","    .option(\"kustoDatabase\", database)\\\n","    .option(\"kustoQuery\", productsQuery).load()"]},{"cell_type":"code","execution_count":null,"id":"dca0ec6b-628f-4e91-9c60-5d7b1e78e0ed","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["### Verifying token availability\n","import requests\n","\n","# Define a simple test query\n","test_query = \"['inventory'] | take 1\"\n","\n","# Define the request headers with the access token\n","headers = {\n","    \"Authorization\": f\"Bearer {accessToken}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Define the request payload\n","payload = {\n","    \"db\": database,\n","    \"csl\": test_query\n","}\n","\n","# Make the request to the Kusto cluster\n","response = requests.post(f\"{kustoUri}/v1/rest/query\", headers=headers, json=payload)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    print(\"Access token is valid and has the necessary permissions.\")\n","else:\n","    print(f\"Failed to validate access token. Status code: {response.status_code}, Response: {response.text}\")"]},{"cell_type":"markdown","id":"d3363923-eb3e-4c3a-928e-3f304fc85fd0","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Step 2: Perform Exploratory Data Analysis"]},{"cell_type":"markdown","id":"0b764c51-1738-49a5-a8cf-e340518e9d8a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Import libraries\n","\n","Before any analysis, you need to import the required libraries."]},{"cell_type":"code","execution_count":null,"id":"eece5426-68f6-4a0b-a478-2d4381322ac3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Importing required libraries\n","import warnings\n","import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","warnings.filterwarnings(\"ignore\")\n","plt.style.use('fivethirtyeight')\n","import pandas as pd\n","import statsmodels.api as sm\n","import matplotlib\n","matplotlib.rcParams['axes.labelsize'] = 14\n","matplotlib.rcParams['xtick.labelsize'] = 12\n","matplotlib.rcParams['ytick.labelsize'] = 12\n","matplotlib.rcParams['text.color'] = 'k'\n","from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n"]},{"cell_type":"markdown","id":"e48037a1-e1cb-4d33-9aa5-57faea8be1a2","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Display raw data"]},{"cell_type":"code","execution_count":null,"id":"1f672306-d3c7-4f1a-89f7-0851e8a90082","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Display data in dataframes.\n","ordersDf.show()\n","productsDf.show()\n","inventoryDf.show()"]},{"cell_type":"code","execution_count":null,"id":"27bf7c95-671a-4a9e-bf04-f80ad0befa73","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Importing functions\n","from pyspark.sql import functions as F\n","\n","# Filter the orders DataFrame for entries from Chicago\n","chicago_sales_df = ordersDf.filter(ordersDf.store_id == 'CHI')\n","\n","# Join the orders DataFrame with the products DataFrame to include product_id and name\n","sales_chicago_df = (\n","    chicago_sales_df\n","    .join(productsDf, on='product_id', how='inner')  # Join on product_id\n","    .select(\n","        'order_date',\n","        'product_id',\n","        'name',  # Include product name\n","        'price',\n","        'quantity',\n","        (chicago_sales_df.price * chicago_sales_df.quantity).alias('sales')  # Calculate sales\n","    )\n",")\n","\n","# Show the new DataFrame with the additional columns\n","sales_chicago_df.show()"]},{"cell_type":"code","execution_count":null,"id":"9749c9d9-7b66-49ca-ab48-1bef9c771478","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Show distinct product names in the dataframe\n","sales_chicago_df.select(\"name\").distinct().show()"]},{"cell_type":"code","execution_count":null,"id":"660ed236-aa9f-4d03-908a-a43af5e32e98","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["### Pre-processing data\n","# Dropping columns that have no impact. Retaining Order Date and Sales.\n","# cols = ['order_id', 'product_id', 'price', 'order_total']\n","# products = products.drop(*cols)\n","# products.show()\n","# Importing functions\n","from pyspark.sql import functions as F\n","\n","# Filter the orders DataFrame for entries from Chicago\n","chicago_sales_df = ordersDf.filter(ordersDf.store_id == 'CHI')\n","\n","# Join the orders DataFrame with the products DataFrame to include product_id and name\n","sales_chicago_df = (\n","    chicago_sales_df\n","    .join(productsDf, on='product_id', how='inner')  # Join on product_id\n","    .select(\n","        'order_date',\n","        (chicago_sales_df.price * chicago_sales_df.quantity).alias('sales')  # Calculate sales\n","    )\n",")\n","\n","# Show the new DataFrame with only order_date and sales columns\n","sales_chicago_df.show()"]},{"cell_type":"code","execution_count":null,"id":"bd93e71e-4786-461a-b3ae-bc0cdedc0fca","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# products = products.groupBy('product_id')\n","# Sort the DataFrame by 'order_date'\n","sorted_sales_chicago_df = sales_chicago_df.orderBy('order_date')\n","\n","# Show the sorted DataFrame\n","sorted_sales_chicago_df.show()"]},{"cell_type":"markdown","id":"57d0a9e5-6958-4bd2-b55e-fbd4e660adcc","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Data Preparation for ML Experiment"]},{"cell_type":"code","execution_count":null,"id":"62880ccb-d1ba-4717-bf95-667b4e550035","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","\n","# Truncate 'order_date' to the first day of the month and group by it, summing the 'sales'\n","grouped_sales_chicago_df = sorted_sales_chicago_df.withColumn(\n","    'order_date_trunc', F.date_trunc('month', 'order_date')\n",").groupBy('order_date_trunc').agg(F.sum('sales').alias('sales_sum'))"]},{"cell_type":"code","execution_count":null,"id":"300b1861-bf24-40b6-bd7c-db7bc1145c47","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Add 67 months to 'order_date_trunc'\n","adjusted_sales_chicago_df = grouped_sales_chicago_df.withColumn(\n","    'adjusted_order_date', F.expr(\"add_months(order_date_trunc, 67)\")\n",")"]},{"cell_type":"code","execution_count":null,"id":"1c4213ca-6d65-4d73-955b-a80e68435ea5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Select relevant columns\n","final_sales_chicago_df = adjusted_sales_chicago_df.select(\n","    'adjusted_order_date', 'sales_sum'\n",")"]},{"cell_type":"code","execution_count":null,"id":"b7ed65d3-adb6-4bd2-8312-1161be2bffd3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Find the maximum 'adjusted_order_date'\n","max_date = final_sales_chicago_df.agg(F.max('adjusted_order_date')).collect()[0][0]\n","print(\"Maximum adjusted order date:\", max_date)"]},{"cell_type":"code","execution_count":null,"id":"936b6a07-2c91-4385-8c38-83ff3b7ea447","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Show the resulting DataFrame\n","final_sales_chicago_df.show()"]},{"cell_type":"markdown","id":"54f2e3b3-4c12-4a3c-a6b3-44b4ca0cd519","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["##### Demonstrate the impact order date on the sales for all sales in Chicago."]},{"cell_type":"code","execution_count":null,"id":"7eb35b38-c3c6-4bac-b324-de48c8e0cce9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Step 1: Convert the Spark DataFrame to Pandas\n","final_sales_chicago_pd_df = final_sales_chicago_df.toPandas()\n","\n","# Step 2: Plot the impact of 'adjusted_order_date' on 'sales_sum'\n","plt.figure(figsize=(12, 3))\n","plt.plot(final_sales_chicago_pd_df['adjusted_order_date'], final_sales_chicago_pd_df['sales_sum'])\n","\n","# Add labels and title\n","plt.xlabel('Order Date')\n","plt.ylabel('Sales')\n","plt.title('Impact of Order Date on Sales')\n","\n","# Step 3: Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"52ef6009-afe3-49ed-acd9-2a983660aa57","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["### Debugging\n","# Check if the DataFrame has data\n","print(final_sales_chicago_pd_df.head())  # This will print the first few rows to check the contents\n","print(final_sales_chicago_pd_df.info())  # This will show if there are any NaN values or data type issues\n","# Ensure 'adjusted_order_date' is in datetime format\n","final_sales_chicago_pd_df['adjusted_order_date'] = pd.to_datetime(final_sales_chicago_pd_df['adjusted_order_date'])\n","\n","# Sort the DataFrame by date\n","final_sales_chicago_pd_df = final_sales_chicago_pd_df.sort_values('adjusted_order_date')\n","\n","# Check the data after sorting\n","print(final_sales_chicago_pd_df.head())\n"]},{"cell_type":"code","execution_count":null,"id":"82b2d558-6189-4236-86d0-dd1fcb70a487","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Filtering Chicago data\n","sales_chicago_df = ordersDf.filter(ordersDf.store_id == 'CHI')\n","\n","# Creating a 'sales' column by multiplying price and quantity\n","sales_chicago_df = sales_chicago_df.withColumn('sales', sales_chicago_df['price'] * sales_chicago_df['quantity'])\n","\n","# Selecting relevant columns: order_date, sales\n","sales_chicago_df = sales_chicago_df.select('order_date', 'sales')\n","\n","# Converting the DataFrame to Pandas\n","sales_chicago_pd_df = sales_chicago_df.toPandas()\n","\n","# Confirming that 'order_date' is in datetime format\n","sales_chicago_pd_df['order_date'] = pd.to_datetime(sales_chicago_pd_df['order_date'])\n","\n","# Group by 'order_date' and sum the sales\n","sales_chicago_pd_df = sales_chicago_pd_df.groupby('order_date')['sales'].sum().reset_index()\n","\n","# Resample the data to get monthly sales\n","sales_chicago_pd_df.set_index('order_date', inplace=True)\n","sales_chicago_monthly = sales_chicago_pd_df['sales'].resample('MS').sum().reset_index()\n","\n","# Plot sales over time\n","plt.figure(figsize=(12, 6))\n","plt.plot(sales_chicago_monthly['order_date'], sales_chicago_monthly['sales'], marker='o')\n","\n","# Adding labels and title\n","plt.xlabel('Order Date')\n","plt.ylabel('Total Sales')\n","plt.title('Impact of Order Date on Sales in Chicago')\n","\n","plt.show()"]}],"metadata":{"dependencies":{},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
